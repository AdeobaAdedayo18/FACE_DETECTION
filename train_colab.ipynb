{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Facial Emotion Recognition Model Training on Google Colab\n",
        "\n",
        "This notebook trains a CNN model to detect facial emotions from images.\n",
        "\n",
        "## Quick Start:\n",
        "1. Enable GPU: `Runtime` ‚Üí `Change runtime type` ‚Üí `GPU` ‚Üí `Save`\n",
        "2. Upload your dataset (see Step 2 or 3)\n",
        "3. Run all cells in order\n",
        "4. Download the trained model when finished\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Understanding Training Progress\n",
        "\n",
        "When training starts, you'll see:\n",
        "- **Progress bars** filling up: `[=====>...]`\n",
        "- **Epoch numbers** incrementing: Epoch 1/50 ‚Üí 2/50 ‚Üí 3/50...\n",
        "- **Metrics updating**: loss decreases, accuracy increases\n",
        "- **Time per step**: shows training speed\n",
        "\n",
        "**Signs training is working:**\n",
        "‚úÖ Progress bars moving\n",
        "‚úÖ Loss decreasing (starts ~1.8, goes down)\n",
        "‚úÖ Accuracy increasing (starts ~0.25-0.30, goes up)\n",
        "‚úÖ Epoch number incrementing\n",
        "\n",
        "Check the cell output below for real-time updates!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick check: Verify GPU is available (optional)\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SYSTEM CHECK\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    print(f\"‚úÖ GPU is ready! Training will be fast.\")\n",
        "    print(f\"GPU Device: {tf.config.list_physical_devices('GPU')[0]}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Training will be slower (2-4 hours vs 20-60 min)\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install opencv-python pillow -q\n",
        "print(\"‚úÖ Packages installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Quick Setup: Extract Dataset (If Already Uploaded via Files)\n",
        "\n",
        "**If you already uploaded `archive.zip` using the Files section (left sidebar), run this cell to extract it:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract archive.zip that you uploaded via Files section\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/archive.zip'\n",
        "\n",
        "# Check if zip exists\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"‚úÖ Found {zip_path}\")\n",
        "    print(\"üì¶ Extracting archive.zip...\")\n",
        "    \n",
        "    # Extract to /content\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "    \n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "    print(\"\\nüîç Looking for dataset folder...\")\n",
        "    \n",
        "    # Check if train and test are directly in /content\n",
        "    content_dir = '/content'\n",
        "    content_items = os.listdir(content_dir)\n",
        "    \n",
        "    # Check if train and test folders exist directly in /content\n",
        "    if 'train' in content_items and 'test' in content_items:\n",
        "        # Dataset structure: /content/train/ and /content/test/\n",
        "        dataset_path = '/content'\n",
        "        print(f\"‚úÖ Found 'train' and 'test' folders directly in /content\")\n",
        "    else:\n",
        "        # Look for a folder containing train and test\n",
        "        dataset_path = None\n",
        "        extracted_items = [item for item in content_items \n",
        "                          if os.path.isdir(os.path.join(content_dir, item)) \n",
        "                          and item not in ['drive', 'sample_data', '.config']]\n",
        "        \n",
        "        print(f\"Checking folders: {extracted_items}\")\n",
        "        \n",
        "        for item in extracted_items:\n",
        "            item_path = os.path.join(content_dir, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                subdirs = os.listdir(item_path)\n",
        "                if 'train' in subdirs and 'test' in subdirs:\n",
        "                    dataset_path = item_path\n",
        "                    print(f\"‚úÖ Found dataset folder: {item_path}\")\n",
        "                    break\n",
        "        \n",
        "        # If still not found, check nested\n",
        "        if dataset_path is None:\n",
        "            for item in extracted_items:\n",
        "                item_path = os.path.join(content_dir, item)\n",
        "                for subitem in os.listdir(item_path):\n",
        "                    subitem_path = os.path.join(item_path, subitem)\n",
        "                    if os.path.isdir(subitem_path):\n",
        "                        subdirs = os.listdir(subitem_path)\n",
        "                        if 'train' in subdirs and 'test' in subdirs:\n",
        "                            dataset_path = subitem_path\n",
        "                            print(f\"‚úÖ Found nested dataset: {dataset_path}\")\n",
        "                            break\n",
        "                if dataset_path:\n",
        "                    break\n",
        "    \n",
        "    # Verify and set global variable\n",
        "    if dataset_path and os.path.exists(dataset_path):\n",
        "        train_path = os.path.join(dataset_path, 'train')\n",
        "        test_path = os.path.join(dataset_path, 'test')\n",
        "        \n",
        "        if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "            print(f\"\\n‚úÖ Dataset found at: {dataset_path}\")\n",
        "            train_contents = os.listdir(train_path)\n",
        "            test_contents = os.listdir(test_path)\n",
        "            print(f\"   Train folders: {train_contents}\")\n",
        "            print(f\"   Test folders: {test_contents}\")\n",
        "            print(\"‚úÖ Dataset structure looks correct!\")\n",
        "            \n",
        "            # Make it a global variable for use in later cells\n",
        "            globals()['dataset_path'] = dataset_path\n",
        "            print(f\"\\n‚úì dataset_path set to: {dataset_path}\")\n",
        "        else:\n",
        "            print(f\"‚ùå train or test folders not found at {dataset_path}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Could not find dataset with 'train' and 'test' folders.\")\n",
        "        print(\"Current /content structure:\")\n",
        "        print(f\"   {content_items}\")\n",
        "else:\n",
        "    print(f\"‚ùå archive.zip not found at {zip_path}\")\n",
        "    print(\"Please make sure you uploaded archive.zip using the Files section (left sidebar)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Troubleshooting: Check What Was Extracted\n",
        "\n",
        "**If extraction didn't work, run this cell to see what's in /content:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Check what was extracted\n",
        "import os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DIAGNOSTIC: Checking /content directory\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "content_path = '/content'\n",
        "if os.path.exists(content_path):\n",
        "    items = os.listdir(content_path)\n",
        "    print(f\"\\nItems in /content: {items}\\n\")\n",
        "    \n",
        "    for item in items:\n",
        "        item_path = os.path.join(content_path, item)\n",
        "        if os.path.isdir(item_path) and item not in ['drive', '.config']:\n",
        "            print(f\"üìÅ {item}/\")\n",
        "            try:\n",
        "                subitems = os.listdir(item_path)\n",
        "                print(f\"   Contains: {subitems[:10]}...\" if len(subitems) > 10 else f\"   Contains: {subitems}\")\n",
        "                \n",
        "                # Check if this folder has train/test\n",
        "                if 'train' in subitems and 'test' in subitems:\n",
        "                    print(f\"   ‚úÖ This folder has 'train' and 'test' - This is your dataset!\")\n",
        "                    print(f\"   üìç Use: dataset_path = '/content/{item}'\")\n",
        "            except:\n",
        "                pass\n",
        "        elif os.path.isfile(item_path):\n",
        "            print(f\"üìÑ {item}\")\n",
        "else:\n",
        "    print(\"‚ùå /content directory doesn't exist\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Quick Fix: Set Dataset Path (For Already Extracted Data)\n",
        "\n",
        "**Since your train and test folders are already in /content, run this to set the path:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick fix: Set dataset_path for your current structure\n",
        "# Your train and test folders are directly in /content\n",
        "\n",
        "dataset_path = '/content'  # Since train/ and test/ are here\n",
        "\n",
        "# Verify\n",
        "import os\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "    print(f\"‚úÖ dataset_path set to: {dataset_path}\")\n",
        "    print(f\"‚úÖ Train folder exists: {train_path}\")\n",
        "    print(f\"‚úÖ Test folder exists: {test_path}\")\n",
        "    train_emotions = os.listdir(train_path)\n",
        "    test_emotions = os.listdir(test_path)\n",
        "    print(f\"‚úÖ Train emotions: {train_emotions}\")\n",
        "    print(f\"‚úÖ Test emotions: {test_emotions}\")\n",
        "    print(\"\\n‚úÖ Ready to proceed! Run the next cells to load data and train.\")\n",
        "else:\n",
        "    print(\"‚ùå Error: train or test folders not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 1: Upload Your Dataset\n",
        "\n",
        "Choose ONE method below:\n",
        "- **Method A**: Upload ZIP file directly (run the cell below)\n",
        "- **Method B**: Use Google Drive (skip this, go to Step 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# METHOD A: Upload dataset as ZIP file\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload zip file\n",
        "print(\"Click 'Choose Files' and select your archive.zip file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Find and extract the zip file\n",
        "zip_filename = [f for f in uploaded.keys() if f.endswith('.zip')][0]\n",
        "print(f\"\\nüì¶ Extracting {zip_filename}...\")\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n",
        "\n",
        "print(f\"‚úÖ Dataset extracted!\")\n",
        "\n",
        "# Set the dataset path\n",
        "dataset_path = '/content/archive'\n",
        "\n",
        "# Verify it exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"‚úÖ Dataset found at: {dataset_path}\")\n",
        "    print(f\"Contents: {os.listdir(dataset_path)}\")\n",
        "else:\n",
        "    print(f\"‚ùå Dataset not found. Please check the extraction.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 2: OR Use Google Drive (Alternative to Step 1)\n",
        "\n",
        "**Only use this if your dataset is already on Google Drive**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# METHOD B: Use Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to your dataset on Drive\n",
        "# UPDATE THIS PATH to where you uploaded your archive folder\n",
        "dataset_path = '/content/drive/MyDrive/archive'  # ‚¨ÖÔ∏è CHANGE THIS IF NEEDED\n",
        "\n",
        "# Verify it exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"‚úÖ Dataset found at: {dataset_path}\")\n",
        "else:\n",
        "    print(f\"‚ùå Dataset not found at: {dataset_path}\")\n",
        "    print(\"Please update dataset_path above to point to your archive folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 3: Import Libraries and Define Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Emotion labels\n",
        "EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "EMOTION_FOLDER_MAP = {\n",
        "    'angry': 0,\n",
        "    'disgust': 1,\n",
        "    'fear': 2,\n",
        "    'happy': 3,\n",
        "    'sad': 4,\n",
        "    'surprise': 5,\n",
        "    'neutral': 6\n",
        "}\n",
        "\n",
        "def load_images_from_folder(folder_path, emotion_label):\n",
        "    \"\"\"Load images from a specific emotion folder.\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    \n",
        "    if not os.path.exists(folder_path):\n",
        "        return images, labels\n",
        "    \n",
        "    image_files = [f for f in os.listdir(folder_path) \n",
        "                   if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "    \n",
        "    print(f\"  Loading {len(image_files)} images from {os.path.basename(folder_path)}...\")\n",
        "    \n",
        "    for img_file in image_files:\n",
        "        img_path = os.path.join(folder_path, img_file)\n",
        "        try:\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if img is None:\n",
        "                pil_img = Image.open(img_path).convert('L')\n",
        "                img = np.array(pil_img)\n",
        "            img_resized = cv2.resize(img, (48, 48))\n",
        "            img_normalized = img_resized.astype('float32') / 255.0\n",
        "            images.append(img_normalized)\n",
        "            labels.append(emotion_label)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    return images, labels\n",
        "\n",
        "def load_fer2013_data(data_path):\n",
        "    \"\"\"Load FER2013 dataset from folder structure.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LOADING FER2013 DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"ERROR: Dataset folder not found at {data_path}\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    print(f\"Found dataset at: {data_path}\")\n",
        "    \n",
        "    train_path = os.path.join(data_path, 'train')\n",
        "    test_path = os.path.join(data_path, 'test')\n",
        "    \n",
        "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
        "        print(f\"ERROR: 'train' or 'test' folders not found\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    # Load training images\n",
        "    print(\"\\nüìÇ Loading training images...\")\n",
        "    train_images = []\n",
        "    train_labels = []\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for emotion_folder, label in EMOTION_FOLDER_MAP.items():\n",
        "        emotion_path = os.path.join(train_path, emotion_folder)\n",
        "        images, labels = load_images_from_folder(emotion_path, label)\n",
        "        train_images.extend(images)\n",
        "        train_labels.extend(labels)\n",
        "    \n",
        "    print(f\"‚úì Training images loaded in {time.time() - start_time:.2f} seconds\")\n",
        "    \n",
        "    # Load test images\n",
        "    print(\"\\nüìÇ Loading test images...\")\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for emotion_folder, label in EMOTION_FOLDER_MAP.items():\n",
        "        emotion_path = os.path.join(test_path, emotion_folder)\n",
        "        images, labels = load_images_from_folder(emotion_path, label)\n",
        "        test_images.extend(images)\n",
        "        test_labels.extend(labels)\n",
        "    \n",
        "    print(f\"‚úì Test images loaded in {time.time() - start_time:.2f} seconds\")\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    X_train = np.array(train_images, dtype='float32')\n",
        "    y_train = np.array(train_labels, dtype='int32')\n",
        "    X_test = np.array(test_images, dtype='float32')\n",
        "    y_test = np.array(test_labels, dtype='int32')\n",
        "    \n",
        "    # Add channel dimension\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "    X_test = np.expand_dims(X_test, axis=-1)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"‚úì Training samples: {len(X_train):,}\")\n",
        "    print(f\"‚úì Test samples: {len(X_test):,}\")\n",
        "    print(f\"‚úì Image shape: {X_train[0].shape}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "print(\"‚úÖ Functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 4: Load the Dataset\n",
        "\n",
        "**Make sure you've set `dataset_path` in Step 1 or Step 2 above!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Make sure dataset_path is set from Step 1 or Step 2 above!\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_fer2013_data(dataset_path)\n",
        "\n",
        "if X_train is None:\n",
        "    print(\"\\n‚ùå Failed to load dataset!\")\n",
        "    print(\"Please check:\")\n",
        "    print(\"1. Did you run Step 1 (upload ZIP) OR Step 2 (mount Drive)?\")\n",
        "    print(\"2. Is dataset_path correctly set?\")\n",
        "    print(\"3. Does the dataset have 'train' and 'test' folders?\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Step 5: Build the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(input_shape=(48, 48, 1), num_classes=7):\n",
        "    \"\"\"Build CNN model for facial emotion recognition.\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        # First convolutional block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        # Second convolutional block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        # Third convolutional block\n",
        "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        # Flatten and dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        \n",
        "        # Output layer\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "print(\"Building CNN model...\")\n",
        "model = build_model()\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display architecture\n",
        "print(\"\\nModel Architecture:\")\n",
        "model.summary()\n",
        "print(\"\\n‚úÖ Model built and ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 6: START TRAINING! \n",
        "\n",
        "**This is the cell that actually trains your model. Run this and watch the progress bars!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'face_emotionModel.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"This may take 20-60 minutes with GPU, or 2-4 hours with CPU only.\")\n",
        "print(\"Watch for progress bars [=====>...] and metrics updating!\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# THIS IS WHERE TRAINING HAPPENS!\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Training completed in {training_time/60:.2f} minutes\")\n",
        "\n",
        "# Load best weights if saved\n",
        "if os.path.exists('face_emotionModel.h5'):\n",
        "    model.load_weights('face_emotionModel.h5')\n",
        "    print(\"‚úÖ Loaded best model weights\")\n",
        "\n",
        "# Evaluate final model\n",
        "print(\"\\nEvaluating model on test set...\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ TRAINING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Model saved as: face_emotionModel.h5\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the trained model\n",
        "from google.colab import files\n",
        "\n",
        "files.download('face_emotionModel.h5')\n",
        "print(\"‚úÖ Model download initiated!\")\n",
        "print(\"üì• Check your Downloads folder for 'face_emotionModel.h5'\")\n",
        "print(\"üìÅ Then copy it to your FACE_DETECTION project folder!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
